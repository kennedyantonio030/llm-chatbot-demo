{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_API_KEY = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Fetches the latest version of this prompt\n",
    "prompt = hub.pull(\"wfh/langsmith-agent-prompt:5d466cbc\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    DuckDuckGoSearchResults(\n",
    "        name=\"duck_duck_go\"\n",
    "    ),  # General internet search using DuckDuckGo\n",
    "]\n",
    "\n",
    "llm_with_tools = llm.bind_functions(tools)\n",
    "\n",
    "runnable_agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=runnable_agent, tools=tools, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is Tesla?\",\n",
    "    \"What's Elon Musk's Net Worth?\",\n",
    "    \"When was Llama-v2 released?\",\n",
    "    \"What is the langsmith cookbook?\",\n",
    "    \"When did langchain first announce the hub?\",\n",
    "]\n",
    "\n",
    "results = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What is Tesla?',\n",
       "  'output': \"Tesla, Inc. is an American electric vehicle and clean energy company. It was founded in 2003 by a group of engineers who wanted to prove that electric cars could be better than gasoline-powered cars. Tesla is known for its innovative electric vehicles, such as the Model S, Model 3, Model X, and Model Y, as well as its energy storage products like the Powerwall and Powerpack. The company is also involved in solar energy through its acquisition of SolarCity. Tesla's mission is to accelerate the world's transition to sustainable energy.\"},\n",
       " {'input': \"What's Elon Musk's Net Worth?\",\n",
       "  'output': \"According to Forbes, Elon Musk's net worth is estimated to be $240.7 billion as of July 2023. Bloomberg and Forbes rank him as the world's richest person, with estimates ranging from $198 billion to $220 billion as of November 2023.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a LangSmith Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "    \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",\n",
    "    \"LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain\",\n",
    "    \"July 18, 2023\",\n",
    "    \"The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications.\",\n",
    "    \"September 5, 2023\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"agent-qa-{unique_id}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name,\n",
    "    description=\"An example dataset of questions over the LangSmith documentation.\",\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"input\": query} for query in inputs],\n",
    "    outputs=[{\"output\": answer} for answer in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, AgentType, initialize_agent, load_tools\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Since chains can be stateful (e.g. they can have memory), we provide\n",
    "# a way to initialize a new chain for each row in the dataset. This is done\n",
    "# by passing in a factory function that returns a new chain for each row.\n",
    "def create_agent(prompt, llm_with_tools):\n",
    "    runnable_agent = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "                x[\"intermediate_steps\"]\n",
    "            ),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm_with_tools\n",
    "        | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "    return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "@run_evaluator\n",
    "def check_not_idk(run: Run, example: Example):\n",
    "    \"\"\"Illustration of a custom evaluator.\"\"\"\n",
    "    agent_response = run.outputs[\"output\"]\n",
    "    if \"don't know\" in agent_response or \"not sure\" in agent_response:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1\n",
    "    # You can access the dataset labels in example.outputs[key]\n",
    "    # You can also access the model inputs in run.inputs[key]\n",
    "    return EvaluationResult(\n",
    "        key=\"not_uncertain\",\n",
    "        score=score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "        # Measure the embedding distance between the output and the reference answer\n",
    "        # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings())\n",
    "        EvaluatorType.EMBEDDING_DISTANCE,\n",
    "        # Grade whether the output satisfies the stated criteria.\n",
    "        # You can select a default one such as \"helpfulness\" or provide your own.\n",
    "        RunEvalConfig.LabeledCriteria(\"helpfulness\"),\n",
    "        # The LabeledScoreString evaluator outputs a score on a scale from 1-10.\n",
    "        # You can use default criteria or write our own rubric\n",
    "        RunEvalConfig.LabeledScoreString(\n",
    "            {\n",
    "                \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 3: The answer has minor relevance but does not align with the reference.\n",
    "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "            },\n",
    "            normalize_by=10,\n",
    "        ),\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[check_not_idk],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# We will test this version of the prompt\n",
    "prompt = hub.pull(\"wfh/langsmith-agent-prompt:798e7324\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'runnable-agent-test-5d466cbc-706e4766' at:\n",
      "https://smith.langchain.com/o/ec3ee2ee-c547-52f1-bb9a-30955085cc0b/datasets/f31cb9c7-1279-47cf-bc9f-9c02908d3092/compare?selectedSessions=c00f3f1b-8a35-4ede-b9b0-eb92d91d3281\n",
      "\n",
      "View all tests for Dataset agent-qa-706e4766 at:\n",
      "https://smith.langchain.com/o/ec3ee2ee-c547-52f1-bb9a-30955085cc0b/datasets/f31cb9c7-1279-47cf-bc9f-9c02908d3092\n",
      "[>                                                 ] 0/5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianroepke/Projects/deadpool-llm/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-22' coro=<AsyncDDGS.__aexit__() running at /Users/brianroepke/Projects/deadpool-llm/venv/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search_async.py:46>>\n",
      "/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:678: RuntimeWarning: coroutine 'AsyncDDGS.__aexit__' was never awaited\n",
      "  self._ready.clear()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-2' coro=<AsyncCurl._force_timeout() running at /Users/brianroepke/Projects/deadpool-llm/venv/lib/python3.11/site-packages/curl_cffi/aio.py:168> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-27' coro=<AsyncDDGS.__aexit__() running at /Users/brianroepke/Projects/deadpool-llm/venv/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search_async.py:46>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-5' coro=<AsyncCurl._force_timeout() running at /Users/brianroepke/Projects/deadpool-llm/venv/lib/python3.11/site-packages/curl_cffi/aio.py:168> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-17' coro=<AsyncDDGS.__aexit__() running at /Users/brianroepke/Projects/deadpool-llm/venv/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search_async.py:46>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-8' coro=<AsyncCurl._force_timeout() running at /Users/brianroepke/Projects/deadpool-llm/venv/lib/python3.11/site-packages/curl_cffi/aio.py:168> wait_for=<Future pending cb=[Task.__wakeup()]>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 5/5"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.embedding_cosine_distance</th>\n",
       "      <th>feedback.helpfulness</th>\n",
       "      <th>feedback.score_string:accuracy</th>\n",
       "      <th>feedback.not_uncertain</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ac50369-9c90-4951-adc8-3acc5c514932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.195385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.655407</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.547723</td>\n",
       "      <td>0.123317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.562278</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.701448</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138944</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.259131</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.153003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.043874</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.303560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.159218</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.113362</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.correctness  feedback.embedding_cosine_distance  \\\n",
       "count               5.000000                            5.000000   \n",
       "unique                   NaN                                 NaN   \n",
       "top                      NaN                                 NaN   \n",
       "freq                     NaN                                 NaN   \n",
       "mean                0.600000                            0.195385   \n",
       "std                 0.547723                            0.123317   \n",
       "min                 0.000000                            0.042215   \n",
       "25%                 0.000000                            0.138944   \n",
       "50%                 1.000000                            0.153003   \n",
       "75%                 1.000000                            0.303560   \n",
       "max                 1.000000                            0.339202   \n",
       "\n",
       "        feedback.helpfulness  feedback.score_string:accuracy  \\\n",
       "count                    5.0                        5.000000   \n",
       "unique                   NaN                             NaN   \n",
       "top                      NaN                             NaN   \n",
       "freq                     NaN                             NaN   \n",
       "mean                     1.0                        0.940000   \n",
       "std                      0.0                        0.134164   \n",
       "min                      1.0                        0.700000   \n",
       "25%                      1.0                        1.000000   \n",
       "50%                      1.0                        1.000000   \n",
       "75%                      1.0                        1.000000   \n",
       "max                      1.0                        1.000000   \n",
       "\n",
       "        feedback.not_uncertain error  execution_time  \\\n",
       "count                      5.0     0        5.000000   \n",
       "unique                     NaN     0             NaN   \n",
       "top                        NaN   NaN             NaN   \n",
       "freq                       NaN   NaN             NaN   \n",
       "mean                       1.0   NaN        7.655407   \n",
       "std                        0.0   NaN        2.562278   \n",
       "min                        1.0   NaN        5.701448   \n",
       "25%                        1.0   NaN        6.259131   \n",
       "50%                        1.0   NaN        7.043874   \n",
       "75%                        1.0   NaN        7.159218   \n",
       "max                        1.0   NaN       12.113362   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      5  \n",
       "unique                                     5  \n",
       "top     1ac50369-9c90-4951-adc8-3acc5c514932  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "from langchain.smith import arun_on_dataset, run_on_dataset\n",
    "\n",
    "chain_results = run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=functools.partial(\n",
    "        create_agent, prompt=prompt, llm_with_tools=llm_with_tools\n",
    "    ),\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"runnable-agent-test-5d466cbc-{unique_id}\",\n",
    "    # Project metadata communicates the experiment parameters,\n",
    "    # Useful for reviewing the test results\n",
    "    project_metadata={\n",
    "        \"env\": \"testing-notebook\",\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"prompt\": \"5d466cbc\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.\n",
    "# These are logged as warnings here and captured as errors in the tracing UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = client.list_runs(project_name=chain_results[\"project_name\"], execution_order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correctness': {'n': 5, 'avg': 0.6},\n",
       " 'embedding_cosine_distance': {'n': 5, 'avg': 0.19536},\n",
       " 'helpfulness': {'n': 5, 'avg': 1.0},\n",
       " 'not_uncertain': {'n': 5, 'avg': 1.0},\n",
       " 'score_string:accuracy': {'n': 5, 'avg': 0.9400000000000001}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After some time, these will be populated.\n",
    "client.read_project(project_name=chain_results[\"project_name\"]).feedback_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
