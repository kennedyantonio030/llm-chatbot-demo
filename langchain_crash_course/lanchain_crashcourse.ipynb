{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c26845d",
   "metadata": {},
   "source": [
    "# Langchain crash course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a6ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed0dc6a",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa352d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Maharaja's Palace\" or \"Taj Mahal Bistro\" \n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "name = llm.invoke(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56e8581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Spice Palace\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782a2dd",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a306b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af406b92",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba65c213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"cuisine\": \"Mexican\",\n",
      "    \"text\": \"\\n\\n\\\"Cantina de Oro\\\" \"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "results = chain.invoke(\"Mexican\")\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ccee75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Mexican food. Suggest a fency name for this.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\n",
      "    \"cuisine\": \"Mexican\",\n",
      "    \"text\": \"\\n\\n\\\"La Casa Del Sabor\\\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
    "results = chain.invoke(\"Mexican\")\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21098937",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a98d9f",
   "metadata": {},
   "source": [
    "#### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9fd9a79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian\n",
      "('\\n'\n",
      " '\\n'\n",
      " '1. Spicy Chicken Curry\\n'\n",
      " '2. Vegetable Samosas\\n'\n",
      " '3. Lamb Vindaloo\\n'\n",
      " '4. Tandoori Chicken\\n'\n",
      " '5. Aloo Gobi (Potato and Cauliflower Curry)\\n'\n",
      " '6. Chana Masala (Chickpea Curry)\\n'\n",
      " '7. Palak Paneer (Spinach and Cheese Curry)\\n'\n",
      " '8. Naan Bread\\n'\n",
      " '9. Mango Lassi (Mango Yogurt Drink)\\n'\n",
      " '10. Vegetable Biryani\\n'\n",
      " '11. Butter Chicken\\n'\n",
      " '12. Onion Bhaji (Fried Onion Fritters)\\n'\n",
      " '13. Rogan Josh (Lamb Stew)\\n'\n",
      " '14. Dal Makhani (Creamy Lentil Curry)\\n'\n",
      " '15. Gulab Jamun (Indian Sweet Dessert)')\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "\n",
    "results = chain.invoke(\"Indian\")\n",
    "print(results[\"input\"])\n",
    "pprint(results[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386d05c",
   "metadata": {},
   "source": [
    "#### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49dc0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dea8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec1be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4653c540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Indian'\n",
      "'\\n\"Spice Palace\"'\n",
      "('\\n'\n",
      " '\\n'\n",
      " '1. Butter Chicken: A classic North Indian dish made with tender chicken in a '\n",
      " 'creamy and flavorful tomato-based sauce.\\n'\n",
      " '\\n'\n",
      " '2. Lamb Vindaloo: A spicy Goan curry made with chunks of tender lamb cooked '\n",
      " 'in a tangy and spicy sauce.\\n'\n",
      " '\\n'\n",
      " '3. Vegetable Biryani: Fragrant basmati rice cooked with mixed vegetables, '\n",
      " 'herbs, and spices.\\n'\n",
      " '\\n'\n",
      " '4. Tandoori Chicken: Marinated chicken cooked in a clay oven, served with a '\n",
      " 'side of naan bread.\\n'\n",
      " '\\n'\n",
      " '5. Chana Masala: A popular vegetarian dish made with chickpeas cooked in a '\n",
      " 'spicy and tangy tomato-based sauce.\\n'\n",
      " '\\n'\n",
      " '6. Palak Paneer: Creamy spinach and cottage cheese curry, a staple in North '\n",
      " 'Indian cuisine.\\n'\n",
      " '\\n'\n",
      " '7. Dal Makhani: Slow-cooked black lentils in a rich and creamy tomato-based '\n",
      " 'sauce.\\n'\n",
      " '\\n'\n",
      " '8. Chicken Tikka Masala: Grilled chicken pieces cooked in a creamy and '\n",
      " 'aromatic tomato-based sauce.\\n'\n",
      " '\\n'\n",
      " '9. Aloo Gobi: A simple and flavorful dish made with potatoes and cauliflower '\n",
      " 'cooked with spices.\\n'\n",
      " '\\n'\n",
      " '10. Garlic Naan: Fluffy flatbread topped with minced garlic and cooked in a '\n",
      " 'clay oven.\\n'\n",
      " '\\n'\n",
      " '11. Mango Lassi: A refreshing yogurt-based drink made with mango puree and '\n",
      " 'spices.\\n'\n",
      " '\\n'\n",
      " '12.')\n"
     ]
    }
   ],
   "source": [
    "results = chain.invoke({\"cuisine\": \"Indian\"})\n",
    "pprint(results[\"cuisine\"])\n",
    "pprint(results[\"restaurant_name\"])\n",
    "pprint(results[\"menu_items\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069a75e",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e6ee719",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY = os.environ.get('SERPAPI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b2c6b",
   "metadata": {},
   "source": [
    "#### serpapi and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddcab683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentType, create_react_agent, load_tools, AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a71e6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fec4212d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for the answer using a search engine.\n",
      "Action: Search\n",
      "Action Input: \"GDP of US in 2022\"\u001b[0m\u001b[36;1m\u001b[1;3m$25.46 trillion\u001b[0m\u001b[32;1m\u001b[1;3mI should use a calculator to convert the GDP from dollars to another currency if needed.\n",
      "Action: Calculator\n",
      "Action Input: $25.46 trillion\u001b[0m\u001b[33;1m\u001b[1;3mAnswer: 25460000000000.0\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The GDP of US in 2022 was $25.46 trillion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What was the GDP of US in 2022?',\n",
       " 'output': 'The GDP of US in 2022 was $25.46 trillion.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "query = \"What was the GDP of US in 2022?\"\n",
    "\n",
    "# Let's test it out!\n",
    "agent_executor.invoke(input={\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd3a12",
   "metadata": {},
   "source": [
    "#### Wikipedia and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d06ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# install this package: pip install wikipedia\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "query = \"When was Elon musk born? What is the square root of his age as of 2023?\"\n",
    "\n",
    "# Let's test it out!\n",
    "agent_executor.invoke(input={\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.211102550927978 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be7ee7",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871492be",
   "metadata": {},
   "source": [
    "#### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eea298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.invoke(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = chain.invoke(\"Arabic\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc88888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a88b5b",
   "metadata": {},
   "source": [
    "#### ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ddd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.invoke(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c80b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.invoke(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07342f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.invoke(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e459d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa3abd",
   "metadata": {},
   "source": [
    "#### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460eb33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "convo.invoke(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.invoke(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b24745",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.invoke(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd907d",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in convo.stream(\"Write me a song about goldfish on the moon\"):\n",
    "    print(chunk[\"history\"])\n",
    "    # print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d61341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "# Set up the prompt template\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Set up the local instace of the LLM\n",
    "local_path = \"/Users/brianroepke/Library/Application Support/nomic.ai/GPT4All/mistral-7b-openorca.Q4_0.gguf\"  # noqa: E501\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "memory = ConversationBufferMemory()\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True, streaming=True)  # noqa: E501\n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "llm_chain = ConversationChain(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm_chain.stream(\"Write me a song about sparkling water.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292a00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
